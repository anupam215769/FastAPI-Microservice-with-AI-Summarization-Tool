# FastAPI Microservice with AI Summarization Tool

A production-ready microservice built with FastAPI for text summarization using an open-source AI model (DistilBART).

## Features

- **Query Endpoint** (`GET /query`):
  - Accepts a user query via a `q` query parameter.
  - Returns a structured JSON response, acknowledging the user’s query.

- **Summarize Endpoint** (`POST /summarize`):
  - Accepts a JSON body containing long-form text, optional `max_length`, and `min_length` parameters.
  - Returns a concise summary generated by a free Hugging Face model.

## Requirements

- Python 3.11+
- See [requirements.txt](./requirements.txt) for a list of Python dependencies.

## Setup & Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/anupam215769/FastAPI-Microservice-with-AI-Summarization-Tool.git
   cd FastAPI-Microservice-with-AI-Summarization-Tool.git
2. **Install dependencies**:
    ```bash
    pip install --no-cache-dir -r requirements.txt
3.  **Run the FastAPI app**:
    ```bash
    uvicorn main:app --host 0.0.0.0 --port 8000

## Usage

- **Query**:
    ```bash
    curl "http://127.0.0.1:8000/query?q=Your+Query+Here"
    
- **Summarize**:
    ```bash
    curl -X POST -H "Content-Type: application/json" -d '{"text": "Your Text for Summarization Here", "max_length": 60, "min_length": 20}' http://127.0.0.1:8000/summarize

## Docker Deployment

1. **Build the Docker image**:
    ```bash
    docker build -t fastapi-summarization-tool .
  
2. **Run the container**:
    ```bash
    docker run -p 80:80 fastapi-summarization-tool

The service will now be available at `http://127.0.0.1:80`

## Testing
- **Run tests**:
    ```bash
    pytest test_main.py
  
## Design & Architectural Decisions

1. **Model Loading**:
    - The summarizer is loaded once during the FastAPI startup event, preventing repeated loading overhead on each request.
2. **API Structure**:
   - A simple /**query** endpoint allows you to test or build additional logic for your microservice.
   - A /**summarize** endpoint demonstrates real-time inference with open-source models.
3. **Scalability**:
   - FastAPI’s asynchronous capabilities and Uvicorn’s support for multiple workers in production can scale horizontally with container tools like Docker
4. **Security & Validation**:
   - Pydantic models are used to validate request bodies.
   - Error handling deals with invalid inputs and inference errors gracefully.

## Limitations & Performance
- **Inference Times**:
  - Running on CPU can be slower for large texts. For faster inference, a GPU-enabled environment is recommended (e.g., CUDA drivers, `nvidia-docker` containers).

- **Memory Constraints**:
  - Summarization models can be memory-intensive for very large inputs.
- **Prompt Length**:
  - The model has practical limits on how long the input text can be; large texts may need chunking or specialized solutions.
